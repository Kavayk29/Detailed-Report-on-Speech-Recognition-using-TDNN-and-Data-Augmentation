{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root folder that contains subfolders with .wav files\n",
    "root_folder = \"processed_recorded_audio\"\n",
    "\n",
    "# Initialize an empty list to store file paths and categories\n",
    "data = []\n",
    "\n",
    "# Iterate through each subfolder\n",
    "for subfolder in os.listdir(root_folder):\n",
    "    subfolder_path = os.path.join(root_folder, subfolder)\n",
    "\n",
    "    # Check if the path is a directory (subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Iterate through all .wav files in the subfolder\n",
    "        for wav_file in os.listdir(subfolder_path):\n",
    "            if wav_file.endswith(\".wav\"):\n",
    "                # Get the full path of the .wav file\n",
    "                file_path = os.path.join(subfolder_path, wav_file)\n",
    "                \n",
    "                # Append the file path and category (subfolder name) to the data list\n",
    "                data.append([file_path, subfolder])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "finaldf = pd.DataFrame(data, columns=[\"file_path\", \"category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>processed_recorded_audio\\Backward\\Backward_01.wav</td>\n",
       "      <td>Backward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>processed_recorded_audio\\Backward\\Backward_02.wav</td>\n",
       "      <td>Backward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processed_recorded_audio\\Backward\\Backward_03.wav</td>\n",
       "      <td>Backward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>processed_recorded_audio\\Backward\\Backward_04.wav</td>\n",
       "      <td>Backward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>processed_recorded_audio\\Backward\\Backward_05.wav</td>\n",
       "      <td>Backward</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  category\n",
       "0  processed_recorded_audio\\Backward\\Backward_01.wav  Backward\n",
       "1  processed_recorded_audio\\Backward\\Backward_02.wav  Backward\n",
       "2  processed_recorded_audio\\Backward\\Backward_03.wav  Backward\n",
       "3  processed_recorded_audio\\Backward\\Backward_04.wav  Backward\n",
       "4  processed_recorded_audio\\Backward\\Backward_05.wav  Backward"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (810640648.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[159], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    data augmentation\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import random\n",
    "import soundfile as sf  # New import for saving audio\n",
    "\n",
    "# Load an audio file\n",
    "def load_audio(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    return audio, sr\n",
    "\n",
    "# Add white noise\n",
    "def add_noise(audio, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(audio))\n",
    "    return audio + noise_factor * noise\n",
    "\n",
    "# Change speed\n",
    "def change_speed(audio, speed_factor=1.5):\n",
    "    return librosa.effects.time_stretch(audio,rate=speed_factor)\n",
    "\n",
    "# Change pitch\n",
    "def change_pitch(audio, sr, n_steps=2):\n",
    "    return librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
    "\n",
    "# Shift time\n",
    "def time_shift(audio, shift_max=0.2):\n",
    "    shift = int(len(audio) * random.uniform(-shift_max, shift_max))\n",
    "    return np.roll(audio, shift)\n",
    "\n",
    "# Adjust volume\n",
    "def adjust_volume(audio, gain_factor=1.5):\n",
    "    return audio * gain_factor\n",
    "\n",
    "# Reverse audio\n",
    "def reverse_audio(audio):\n",
    "    return np.flip(audio)\n",
    "\n",
    "# Function to save augmented audio and add new rows to the dataframe\n",
    "def augment_audio_and_update_df(finaldf, output_folder):\n",
    "    new_rows = []\n",
    "    \n",
    "    for index, row in finaldf.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        category = row['category']\n",
    "        audio, sr = load_audio(file_path)\n",
    "        \n",
    "        # Create category subfolder if it doesn't exist\n",
    "        category_folder = os.path.join(output_folder, category)\n",
    "        os.makedirs(category_folder, exist_ok=True)\n",
    "\n",
    "        # Augmentation techniques\n",
    "        augmentations = [\n",
    "            ('original', audio),\n",
    "            ('noise', add_noise(audio)),\n",
    "            ('speed_up', change_speed(audio, speed_factor=1.2)),\n",
    "            ('speed_down', change_speed(audio, speed_factor=0.8)),\n",
    "            ('pitch_up', change_pitch(audio, sr, n_steps=2)),\n",
    "            ('pitch_down', change_pitch(audio, sr, n_steps=-2)),\n",
    "            ('shift_forward', time_shift(audio, shift_max=0.2)),\n",
    "            ('shift_backward', time_shift(audio, shift_max=-0.2)),\n",
    "            ('volume_up', adjust_volume(audio, gain_factor=1.5)),\n",
    "            ('volume_down', adjust_volume(audio, gain_factor=0.5)),\n",
    "            ('reversed', reverse_audio(audio))\n",
    "        ]\n",
    "        \n",
    "        # Save augmentations and add new rows\n",
    "        for aug_type, aug_audio in augmentations:\n",
    "            output_file = os.path.join(category_folder, f\"{aug_type}_{index}.wav\")\n",
    "            sf.write(output_file, aug_audio, sr)  # Replaced librosa.output.write_wav() with sf.write\n",
    "            \n",
    "            # Append new row data (file_path and category)\n",
    "            new_rows.append({\n",
    "                'file_path': output_file,\n",
    "                'category': category\n",
    "            })\n",
    "    \n",
    "    # Add the new augmented rows to the original dataframe\n",
    "    augmented_df = pd.DataFrame(new_rows)\n",
    "    updated_df = pd.concat([finaldf, augmented_df], ignore_index=True)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "# Example usage\n",
    "output_folder = 'augmented_audio_dataset'  # Output folder to save augmented files\n",
    "updated_df = augment_audio_and_update_df(finaldf, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>augmented_audio_dataset\\Seven\\pitch_up_1029.wav</td>\n",
       "      <td>Seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>augmented_audio_dataset\\two\\pitch_up_1288.wav</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>augmented_audio_dataset\\Forward\\volume_up_311.wav</td>\n",
       "      <td>Forward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>processed_recorded_audio\\visual\\visual_21.wav</td>\n",
       "      <td>visual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>augmented_audio_dataset\\Eight\\shift_backward_1...</td>\n",
       "      <td>Eight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path category\n",
       "0    augmented_audio_dataset\\Seven\\pitch_up_1029.wav    Seven\n",
       "1      augmented_audio_dataset\\two\\pitch_up_1288.wav      two\n",
       "2  augmented_audio_dataset\\Forward\\volume_up_311.wav  Forward\n",
       "3      processed_recorded_audio\\visual\\visual_21.wav   visual\n",
       "4  augmented_audio_dataset\\Eight\\shift_backward_1...    Eight"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df = updated_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18456, 2)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('recorded_data_with_data_aug.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\py310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Processing chunk 3\n",
      "Processing chunk 4\n",
      "Processing chunk 5\n",
      "Processing chunk 6\n",
      "Processing chunk 7\n",
      "Processing chunk 8\n",
      "Processing chunk 9\n",
      "Processing chunk 10\n",
      "Processing chunk 11\n",
      "Processing chunk 12\n",
      "Processing chunk 13\n",
      "Processing chunk 14\n",
      "Processing chunk 15\n",
      "Processing chunk 16\n",
      "Processing chunk 17\n",
      "Processing chunk 18\n",
      "Processing chunk 19\n",
      "Processing chunk 20\n",
      "Processing chunk 21\n",
      "Processing chunk 22\n",
      "Processing chunk 23\n",
      "Processing chunk 24\n",
      "Processing chunk 25\n",
      "Processing chunk 26\n",
      "Processing chunk 27\n",
      "Processing chunk 28\n",
      "Processing chunk 29\n",
      "Processing chunk 30\n",
      "Processing chunk 31\n",
      "Processing chunk 32\n",
      "Processing chunk 33\n",
      "Processing chunk 34\n",
      "Processing chunk 35\n",
      "Processing chunk 36\n",
      "Processing chunk 37\n",
      "Processing chunk 38\n",
      "Processing chunk 39\n",
      "Processing chunk 40\n",
      "Processing chunk 41\n",
      "Processing chunk 42\n",
      "Processing chunk 43\n",
      "Processing chunk 44\n",
      "Processing chunk 45\n",
      "Processing chunk 46\n",
      "Processing chunk 47\n",
      "Processing chunk 48\n",
      "Processing chunk 49\n",
      "Processing chunk 50\n",
      "Processing chunk 51\n",
      "Processing chunk 52\n",
      "Processing chunk 53\n",
      "Processing chunk 54\n",
      "Processing chunk 55\n",
      "Processing chunk 56\n",
      "Processing chunk 57\n",
      "Processing chunk 58\n",
      "Processing chunk 59\n",
      "Processing chunk 60\n",
      "Processing chunk 61\n",
      "Processing chunk 62\n",
      "Processing chunk 63\n",
      "Processing chunk 64\n",
      "Processing chunk 65\n",
      "Processing chunk 66\n",
      "Processing chunk 67\n",
      "Processing chunk 68\n",
      "Processing chunk 69\n",
      "Processing chunk 70\n",
      "Processing chunk 71\n",
      "Processing chunk 72\n",
      "Processing chunk 73\n",
      "Processing chunk 74\n",
      "Processing chunk 75\n",
      "Processing chunk 76\n",
      "Processing chunk 77\n",
      "Processing chunk 78\n",
      "Processing chunk 79\n",
      "Processing chunk 80\n",
      "Processing chunk 81\n",
      "Processing chunk 82\n",
      "Processing chunk 83\n",
      "Processing chunk 84\n",
      "Processing chunk 85\n",
      "Processing chunk 86\n",
      "Processing chunk 87\n",
      "Processing chunk 88\n",
      "Processing chunk 89\n",
      "Processing chunk 90\n",
      "Processing chunk 91\n",
      "Processing chunk 92\n",
      "Processing chunk 93\n",
      "Processing chunk 94\n",
      "Processing chunk 95\n",
      "Processing chunk 96\n",
      "Processing chunk 97\n",
      "Processing chunk 98\n",
      "Processing chunk 99\n",
      "Processing chunk 100\n",
      "Processing chunk 101\n",
      "Processing chunk 102\n",
      "Processing chunk 103\n",
      "Processing chunk 104\n",
      "Processing chunk 105\n",
      "Processing chunk 106\n",
      "Processing chunk 107\n",
      "Processing chunk 108\n",
      "Processing chunk 109\n",
      "Processing chunk 110\n",
      "Processing chunk 111\n",
      "Processing chunk 112\n",
      "Processing chunk 113\n",
      "Processing chunk 114\n",
      "Processing chunk 115\n",
      "Processing chunk 116\n",
      "Processing chunk 117\n",
      "Processing chunk 118\n",
      "Processing chunk 119\n",
      "Processing chunk 120\n",
      "Processing chunk 121\n",
      "Processing chunk 122\n",
      "Processing chunk 123\n",
      "Processing chunk 124\n",
      "Processing chunk 125\n",
      "Processing chunk 126\n",
      "Processing chunk 127\n",
      "Processing chunk 128\n",
      "Processing chunk 129\n",
      "Processing chunk 130\n",
      "Processing chunk 131\n",
      "Processing chunk 132\n",
      "Processing chunk 133\n",
      "Processing chunk 134\n",
      "Processing chunk 135\n",
      "Processing chunk 136\n",
      "Processing chunk 137\n",
      "Processing chunk 138\n",
      "Processing chunk 139\n",
      "Processing chunk 140\n",
      "Processing chunk 141\n",
      "Processing chunk 142\n",
      "Processing chunk 143\n",
      "Processing chunk 144\n",
      "Processing chunk 145\n",
      "Processing chunk 146\n",
      "Processing chunk 147\n",
      "Processing chunk 148\n",
      "Processing chunk 149\n",
      "Processing chunk 150\n",
      "Processing chunk 151\n",
      "Processing chunk 152\n",
      "Processing chunk 153\n",
      "Processing chunk 154\n",
      "Processing chunk 155\n",
      "Processing chunk 156\n",
      "Processing chunk 157\n",
      "Processing chunk 158\n",
      "Processing chunk 159\n",
      "Processing chunk 160\n",
      "Processing chunk 161\n",
      "Processing chunk 162\n",
      "Processing chunk 163\n",
      "Processing chunk 164\n",
      "Processing chunk 165\n",
      "Processing chunk 166\n",
      "Processing chunk 167\n",
      "Processing chunk 168\n",
      "Processing chunk 169\n",
      "Processing chunk 170\n",
      "Processing chunk 171\n",
      "Processing chunk 172\n",
      "Processing chunk 173\n",
      "Processing chunk 174\n",
      "Processing chunk 175\n",
      "Processing chunk 176\n",
      "Processing chunk 177\n",
      "Processing chunk 178\n",
      "Processing chunk 179\n",
      "Processing chunk 180\n",
      "Processing chunk 181\n",
      "Processing chunk 182\n",
      "Processing chunk 183\n",
      "Processing chunk 184\n",
      "Processing chunk 185\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Function to preprocess the audio files\n",
    "def preprocess_audio(audio, sr, target_sr=16000, max_length=5):\n",
    "    # Resample audio if needed\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        sr = target_sr\n",
    "    \n",
    "    # Normalize the audio\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    \n",
    "    # Trim leading and trailing silence\n",
    "    audio, _ = librosa.effects.trim(audio)\n",
    "    \n",
    "    # Calculate the required padding\n",
    "    target_length = target_sr * max_length\n",
    "    if len(audio) < target_length:\n",
    "        padding = target_length - len(audio)\n",
    "        # Pad the audio file\n",
    "        audio = np.pad(audio, (0, padding), 'constant')\n",
    "    else:\n",
    "        # If audio is longer than target length, truncate it\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    return audio, sr\n",
    "\n",
    "# Function to preprocess a chunk of the DataFrame\n",
    "def preprocess_chunk(df_chunk, target_sr=16000, max_length=5):\n",
    "    processed_data = []\n",
    "    \n",
    "    for idx, row in df_chunk.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        category = row['category']\n",
    "        \n",
    "        try:\n",
    "            # Load the audio file\n",
    "            audio, sr = librosa.load(file_path, sr=None)\n",
    "            \n",
    "            # Preprocess the audio\n",
    "            preprocessed_audio, preprocessed_sr = preprocess_audio(audio, sr, target_sr, max_length)\n",
    "            \n",
    "            # Append preprocessed data\n",
    "            processed_data.append({\n",
    "                'file_path': file_path,\n",
    "                'category': category,\n",
    "                'audio': preprocessed_audio,\n",
    "                'sr': preprocessed_sr\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Function to process the DataFrame in chunks\n",
    "def process_dataframe_in_chunks(df, chunk_size=100, target_sr=16000, max_length=5):\n",
    "    processed_chunks = []\n",
    "    \n",
    "    # Split DataFrame into chunks and process each chunk\n",
    "    for chunk_idx, df_chunk in enumerate(np.array_split(updated_df, len(updated_df) // chunk_size + 1)):\n",
    "        print(f\"Processing chunk {chunk_idx + 1}\")\n",
    "        processed_chunk = preprocess_chunk(df_chunk, target_sr, max_length)\n",
    "        processed_chunks.append(processed_chunk)\n",
    "        del df_chunk  # Free memory\n",
    "    \n",
    "    # Concatenate all processed chunks into a single DataFrame\n",
    "    processed_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Example of how to use the function\n",
    "# Assume df has the columns: file_path, category\n",
    "preprocessed_df = process_dataframe_in_chunks(updated_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mfcc features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>augmented_audio_dataset\\Seven\\pitch_up_1029.wav</td>\n",
       "      <td>Seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>augmented_audio_dataset\\two\\pitch_up_1288.wav</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>augmented_audio_dataset\\Forward\\volume_up_311.wav</td>\n",
       "      <td>Forward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>processed_recorded_audio\\visual\\visual_21.wav</td>\n",
       "      <td>visual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>augmented_audio_dataset\\Eight\\shift_backward_1...</td>\n",
       "      <td>Eight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path category\n",
       "0    augmented_audio_dataset\\Seven\\pitch_up_1029.wav    Seven\n",
       "1      augmented_audio_dataset\\two\\pitch_up_1288.wav      two\n",
       "2  augmented_audio_dataset\\Forward\\volume_up_311.wav  Forward\n",
       "3      processed_recorded_audio\\visual\\visual_21.wav   visual\n",
       "4  augmented_audio_dataset\\Eight\\shift_backward_1...    Eight"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features(audio, sr, n_mfcc=13):\n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Take the mean of the MFCC coefficients over time\n",
    "    mfcc_mean = mfcc.mean(axis=1)\n",
    "    return mfcc_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(file_path, target_sr=16000, n_mfcc=13):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # Resample and preprocess audio\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        sr = target_sr\n",
    "    \n",
    "    audio = librosa.util.normalize(audio)\n",
    "    audio, _ = librosa.effects.trim(audio)\n",
    "    \n",
    "    target_length = target_sr * 5  # Max length in seconds (e.g., 5 seconds)\n",
    "    if len(audio) < target_length:\n",
    "        padding = target_length - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), 'constant')\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfcc_features = extract_mfcc_features(audio, sr, n_mfcc)\n",
    "    \n",
    "    return mfcc_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_in_chunks(newdf, chunk_size=100, target_sr=16000, n_mfcc=14):\n",
    "    # Initialize lists to store processed data\n",
    "    mfcc_features_list = []\n",
    "    file_paths = []\n",
    "    \n",
    "    for chunk_idx, df_chunk in enumerate(np.array_split(updated_df, len(updated_df) // chunk_size + 1)):\n",
    "        print(f\"Processing chunk {chunk_idx + 1}\")\n",
    "        \n",
    "        for idx, row in df_chunk.iterrows():\n",
    "            file_path = row['file_path']\n",
    "            \n",
    "            try:\n",
    "                # Process audio file and extract MFCC features\n",
    "                mfcc_features = process_audio(file_path, target_sr, n_mfcc)\n",
    "                mfcc_features_list.append(mfcc_features)\n",
    "                file_paths.append(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "        \n",
    "        # Free memory\n",
    "        del df_chunk\n",
    "\n",
    "    # Convert lists to DataFrame\n",
    "    mfcc_df = pd.DataFrame(mfcc_features_list, columns=[f'mfcc_{i}' for i in range(n_mfcc)])\n",
    "    result_df = pd.DataFrame({'file_path': file_paths}).join(mfcc_df)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\py310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Processing chunk 3\n",
      "Processing chunk 4\n",
      "Processing chunk 5\n",
      "Processing chunk 6\n",
      "Processing chunk 7\n",
      "Processing chunk 8\n",
      "Processing chunk 9\n",
      "Processing chunk 10\n",
      "Processing chunk 11\n",
      "Processing chunk 12\n",
      "Processing chunk 13\n",
      "Processing chunk 14\n",
      "Processing chunk 15\n",
      "Processing chunk 16\n",
      "Processing chunk 17\n",
      "Processing chunk 18\n",
      "Processing chunk 19\n",
      "Processing chunk 20\n",
      "Processing chunk 21\n",
      "Processing chunk 22\n",
      "Processing chunk 23\n",
      "Processing chunk 24\n",
      "Processing chunk 25\n",
      "Processing chunk 26\n",
      "Processing chunk 27\n",
      "Processing chunk 28\n",
      "Processing chunk 29\n",
      "Processing chunk 30\n",
      "Processing chunk 31\n",
      "Processing chunk 32\n",
      "Processing chunk 33\n",
      "Processing chunk 34\n",
      "Processing chunk 35\n",
      "Processing chunk 36\n",
      "Processing chunk 37\n",
      "Processing chunk 38\n",
      "Processing chunk 39\n",
      "Processing chunk 40\n",
      "Processing chunk 41\n",
      "Processing chunk 42\n",
      "Processing chunk 43\n",
      "Processing chunk 44\n",
      "Processing chunk 45\n",
      "Processing chunk 46\n",
      "Processing chunk 47\n",
      "Processing chunk 48\n",
      "Processing chunk 49\n",
      "Processing chunk 50\n",
      "Processing chunk 51\n",
      "Processing chunk 52\n",
      "Processing chunk 53\n",
      "Processing chunk 54\n",
      "Processing chunk 55\n",
      "Processing chunk 56\n",
      "Processing chunk 57\n",
      "Processing chunk 58\n",
      "Processing chunk 59\n",
      "Processing chunk 60\n",
      "Processing chunk 61\n",
      "Processing chunk 62\n",
      "Processing chunk 63\n",
      "Processing chunk 64\n",
      "Processing chunk 65\n",
      "Processing chunk 66\n",
      "Processing chunk 67\n",
      "Processing chunk 68\n",
      "Processing chunk 69\n",
      "Processing chunk 70\n",
      "Processing chunk 71\n",
      "Processing chunk 72\n",
      "Processing chunk 73\n",
      "Processing chunk 74\n",
      "Processing chunk 75\n",
      "Processing chunk 76\n",
      "Processing chunk 77\n",
      "Processing chunk 78\n",
      "Processing chunk 79\n",
      "Processing chunk 80\n",
      "Processing chunk 81\n",
      "Processing chunk 82\n",
      "Processing chunk 83\n",
      "Processing chunk 84\n",
      "Processing chunk 85\n",
      "Processing chunk 86\n",
      "Processing chunk 87\n",
      "Processing chunk 88\n",
      "Processing chunk 89\n",
      "Processing chunk 90\n",
      "Processing chunk 91\n",
      "Processing chunk 92\n",
      "Processing chunk 93\n",
      "Processing chunk 94\n",
      "Processing chunk 95\n",
      "Processing chunk 96\n",
      "Processing chunk 97\n",
      "Processing chunk 98\n",
      "Processing chunk 99\n",
      "Processing chunk 100\n",
      "Processing chunk 101\n",
      "Processing chunk 102\n",
      "Processing chunk 103\n",
      "Processing chunk 104\n",
      "Processing chunk 105\n",
      "Processing chunk 106\n",
      "Processing chunk 107\n",
      "Processing chunk 108\n",
      "Processing chunk 109\n",
      "Processing chunk 110\n",
      "Processing chunk 111\n",
      "Processing chunk 112\n",
      "Processing chunk 113\n",
      "Processing chunk 114\n",
      "Processing chunk 115\n",
      "Processing chunk 116\n",
      "Processing chunk 117\n",
      "Processing chunk 118\n",
      "Processing chunk 119\n",
      "Processing chunk 120\n",
      "Processing chunk 121\n",
      "Processing chunk 122\n",
      "Processing chunk 123\n",
      "Processing chunk 124\n",
      "Processing chunk 125\n",
      "Processing chunk 126\n",
      "Processing chunk 127\n",
      "Processing chunk 128\n",
      "Processing chunk 129\n",
      "Processing chunk 130\n",
      "Processing chunk 131\n",
      "Processing chunk 132\n",
      "Processing chunk 133\n",
      "Processing chunk 134\n",
      "Processing chunk 135\n",
      "Processing chunk 136\n",
      "Processing chunk 137\n",
      "Processing chunk 138\n",
      "Processing chunk 139\n",
      "Processing chunk 140\n",
      "Processing chunk 141\n",
      "Processing chunk 142\n",
      "Processing chunk 143\n",
      "Processing chunk 144\n",
      "Processing chunk 145\n",
      "Processing chunk 146\n",
      "Processing chunk 147\n",
      "Processing chunk 148\n",
      "Processing chunk 149\n",
      "Processing chunk 150\n",
      "Processing chunk 151\n",
      "Processing chunk 152\n",
      "Processing chunk 153\n",
      "Processing chunk 154\n",
      "Processing chunk 155\n",
      "Processing chunk 156\n",
      "Processing chunk 157\n",
      "Processing chunk 158\n",
      "Processing chunk 159\n",
      "Processing chunk 160\n",
      "Processing chunk 161\n",
      "Processing chunk 162\n",
      "Processing chunk 163\n",
      "Processing chunk 164\n",
      "Processing chunk 165\n",
      "Processing chunk 166\n",
      "Processing chunk 167\n",
      "Processing chunk 168\n",
      "Processing chunk 169\n",
      "Processing chunk 170\n",
      "Processing chunk 171\n",
      "Processing chunk 172\n",
      "Processing chunk 173\n",
      "Processing chunk 174\n",
      "Processing chunk 175\n",
      "Processing chunk 176\n",
      "Processing chunk 177\n",
      "Processing chunk 178\n",
      "Processing chunk 179\n",
      "Processing chunk 180\n",
      "Processing chunk 181\n",
      "Processing chunk 182\n",
      "Processing chunk 183\n",
      "Processing chunk 184\n",
      "Processing chunk 185\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Assume df has the columns: file_path, category\n",
    "mfcc_df = process_dataframe_in_chunks(updated_df)\n",
    "\n",
    "# Merge MFCC features with the original DataFrame\n",
    "updated_df = updated_df.merge(mfcc_df, on='file_path', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('recorderd_data_with_data_aug_and_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "      <th>mfcc_0</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>mfcc_6</th>\n",
       "      <th>mfcc_7</th>\n",
       "      <th>mfcc_8</th>\n",
       "      <th>mfcc_9</th>\n",
       "      <th>mfcc_10</th>\n",
       "      <th>mfcc_11</th>\n",
       "      <th>mfcc_12</th>\n",
       "      <th>mfcc_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>augmented_audio_dataset\\Seven\\pitch_up_1029.wav</td>\n",
       "      <td>Seven</td>\n",
       "      <td>-499.928345</td>\n",
       "      <td>9.105547</td>\n",
       "      <td>2.492326</td>\n",
       "      <td>4.310031</td>\n",
       "      <td>1.542680</td>\n",
       "      <td>-0.107071</td>\n",
       "      <td>-1.836942</td>\n",
       "      <td>1.100151</td>\n",
       "      <td>0.928557</td>\n",
       "      <td>0.948992</td>\n",
       "      <td>0.234807</td>\n",
       "      <td>0.320373</td>\n",
       "      <td>1.025592</td>\n",
       "      <td>0.396697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>augmented_audio_dataset\\two\\pitch_up_1288.wav</td>\n",
       "      <td>two</td>\n",
       "      <td>-447.733368</td>\n",
       "      <td>11.961545</td>\n",
       "      <td>5.061387</td>\n",
       "      <td>-0.527899</td>\n",
       "      <td>-1.532452</td>\n",
       "      <td>0.944962</td>\n",
       "      <td>0.123386</td>\n",
       "      <td>-3.872755</td>\n",
       "      <td>-0.613846</td>\n",
       "      <td>-4.191105</td>\n",
       "      <td>-1.026834</td>\n",
       "      <td>-2.010867</td>\n",
       "      <td>0.851715</td>\n",
       "      <td>-0.054870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>augmented_audio_dataset\\Forward\\volume_up_311.wav</td>\n",
       "      <td>Forward</td>\n",
       "      <td>-488.926239</td>\n",
       "      <td>18.606514</td>\n",
       "      <td>-0.690390</td>\n",
       "      <td>0.179542</td>\n",
       "      <td>2.166796</td>\n",
       "      <td>-0.838498</td>\n",
       "      <td>-0.456715</td>\n",
       "      <td>-2.516395</td>\n",
       "      <td>-0.679739</td>\n",
       "      <td>-2.244136</td>\n",
       "      <td>0.659838</td>\n",
       "      <td>2.306940</td>\n",
       "      <td>-2.561234</td>\n",
       "      <td>1.074301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>processed_recorded_audio\\visual\\visual_21.wav</td>\n",
       "      <td>visual</td>\n",
       "      <td>-500.696594</td>\n",
       "      <td>13.200192</td>\n",
       "      <td>-1.008743</td>\n",
       "      <td>4.651892</td>\n",
       "      <td>1.181455</td>\n",
       "      <td>-1.134971</td>\n",
       "      <td>-1.300725</td>\n",
       "      <td>0.395899</td>\n",
       "      <td>-2.419369</td>\n",
       "      <td>-0.320029</td>\n",
       "      <td>-3.210164</td>\n",
       "      <td>0.794875</td>\n",
       "      <td>-0.942491</td>\n",
       "      <td>0.316947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>augmented_audio_dataset\\Eight\\shift_backward_1...</td>\n",
       "      <td>Eight</td>\n",
       "      <td>-479.694580</td>\n",
       "      <td>9.368511</td>\n",
       "      <td>-0.375977</td>\n",
       "      <td>5.127130</td>\n",
       "      <td>-1.506138</td>\n",
       "      <td>-2.261334</td>\n",
       "      <td>-0.785565</td>\n",
       "      <td>-1.900638</td>\n",
       "      <td>-1.586725</td>\n",
       "      <td>-2.909011</td>\n",
       "      <td>-0.681705</td>\n",
       "      <td>-0.533147</td>\n",
       "      <td>-0.656668</td>\n",
       "      <td>-0.388845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path category      mfcc_0  \\\n",
       "0    augmented_audio_dataset\\Seven\\pitch_up_1029.wav    Seven -499.928345   \n",
       "1      augmented_audio_dataset\\two\\pitch_up_1288.wav      two -447.733368   \n",
       "2  augmented_audio_dataset\\Forward\\volume_up_311.wav  Forward -488.926239   \n",
       "3      processed_recorded_audio\\visual\\visual_21.wav   visual -500.696594   \n",
       "4  augmented_audio_dataset\\Eight\\shift_backward_1...    Eight -479.694580   \n",
       "\n",
       "      mfcc_1    mfcc_2    mfcc_3    mfcc_4    mfcc_5    mfcc_6    mfcc_7  \\\n",
       "0   9.105547  2.492326  4.310031  1.542680 -0.107071 -1.836942  1.100151   \n",
       "1  11.961545  5.061387 -0.527899 -1.532452  0.944962  0.123386 -3.872755   \n",
       "2  18.606514 -0.690390  0.179542  2.166796 -0.838498 -0.456715 -2.516395   \n",
       "3  13.200192 -1.008743  4.651892  1.181455 -1.134971 -1.300725  0.395899   \n",
       "4   9.368511 -0.375977  5.127130 -1.506138 -2.261334 -0.785565 -1.900638   \n",
       "\n",
       "     mfcc_8    mfcc_9   mfcc_10   mfcc_11   mfcc_12   mfcc_13  \n",
       "0  0.928557  0.948992  0.234807  0.320373  1.025592  0.396697  \n",
       "1 -0.613846 -4.191105 -1.026834 -2.010867  0.851715 -0.054870  \n",
       "2 -0.679739 -2.244136  0.659838  2.306940 -2.561234  1.074301  \n",
       "3 -2.419369 -0.320029 -3.210164  0.794875 -0.942491  0.316947  \n",
       "4 -1.586725 -2.909011 -0.681705 -0.533147 -0.656668 -0.388845  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18456, 16)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezing few layers of the trained model and then training last few layers and adding new layers as well to fine tunr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, Flatten,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MFCC features and labels from the dataframe\n",
    "X = updated_df.iloc[:, 2:].values  # Extract MFCC columns mfcc_0 to mfcc_13\n",
    "y = updated_df['category'].values  # Extract labels (categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the category labels to numeric form\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# One-hot encode the labels (since we have 36 categories)\n",
    "y_one_hot = to_categorical(y_encoded, num_classes=36)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the MFCC features to include a 'channel' dimension if required by the model\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_val = np.expand_dims(X_val, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_35 (Conv1D)          (None, 14, 128)           768       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 14, 128)          512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 14, 128)           0         \n",
      "                                                                 \n",
      " conv1d_36 (Conv1D)          (None, 14, 256)           98560     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 14, 256)           0         \n",
      "                                                                 \n",
      " conv1d_37 (Conv1D)          (None, 14, 256)           196864    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 14, 256)           0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 3584)              0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 512)               1835520   \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 36)                9252      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,274,852\n",
      "Trainable params: 2,273,572\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing the saved model (.pb file)\n",
    "saved_model_dir = 'model'\n",
    "\n",
    "# Load the model from the SavedModel format\n",
    "model = tf.keras.models.load_model(saved_model_dir)\n",
    "\n",
    "# Display the model summary to understand its structure\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 14, 256])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the last layer before flatten\n",
    "x = model.layers[8].output.shape \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d_35: Non-trainable\n",
      "batch_normalization: Non-trainable\n",
      "dropout_30: Non-trainable\n",
      "conv1d_36: Non-trainable\n",
      "batch_normalization_1: Non-trainable\n",
      "dropout_31: Non-trainable\n",
      "conv1d_37: Non-trainable\n",
      "batch_normalization_2: Non-trainable\n",
      "dropout_32: Non-trainable\n",
      "flatten_7: Non-trainable\n",
      "dense_20: Non-trainable\n",
      "dropout_33: Trainable\n",
      "dense_21: Trainable\n",
      "dropout_34: Trainable\n",
      "dense_22: Trainable\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_35_input (InputLayer  [(None, 14, 1)]          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_35 (Conv1D)          (None, 14, 128)           768       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 14, 128)          512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 14, 128)           0         \n",
      "                                                                 \n",
      " conv1d_36 (Conv1D)          (None, 14, 256)           98560     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 14, 256)           0         \n",
      "                                                                 \n",
      " conv1d_37 (Conv1D)          (None, 14, 256)           196864    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 14, 256)           0         \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (None, 14, 512)           655872    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 14, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_9 (ReLU)              (None, 14, 512)           0         \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 14, 512)           0         \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          (None, 14, 512)           786944    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 14, 512)          2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_10 (ReLU)             (None, 14, 512)           0         \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 14, 512)           0         \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (None, 14, 512)           786944    \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 14, 512)          2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_11 (ReLU)             (None, 14, 512)           0         \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 14, 512)           0         \n",
      "                                                                 \n",
      " global_average_pooling1d_4   (None, 512)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 36)                9252      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,430,820\n",
      "Trainable params: 5,128,996\n",
      "Non-trainable params: 301,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, ReLU, Dense, Dropout, Flatten, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the pre-trained model\n",
    "# model = load_model('path_to_your_trained_model.pb')  # Adjust if needed\n",
    "\n",
    "# Freeze all layers except the last few\n",
    "for layer in model.layers[:-4]:  # Adjust the number of layers to freeze\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check if layers are set to non-trainable\n",
    "for layer in model.layers:\n",
    "    print(f\"{layer.name}: {'Trainable' if layer.trainable else 'Non-trainable'}\")\n",
    "\n",
    "# Create a new model with the same input as the original\n",
    "inputs = model.input\n",
    "\n",
    "# Take output from a deeper layer (before Flatten)\n",
    "x = model.layers[8].output  # Adjust to the last Conv1D layer you want to keep\n",
    "\n",
    "# Add new Conv1D layers for more complex patterns\n",
    "x = Conv1D(filters=512, kernel_size=5, padding='same', strides=1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(filters=512, kernel_size=3, padding='same', strides=1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(filters=512, kernel_size=3, padding='same', strides=1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Add Global Average Pooling\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Fully connected Dense layers for fine-tuning\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "\n",
    "# Output layer for classification (36 categories)\n",
    "outputs = Dense(36, activation='softmax')(x)\n",
    "\n",
    "# Create a new model\n",
    "fine_tuned_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the new model\n",
    "optimizer = Adam(learning_rate=1e-5)  # Lower learning rate for fine-tuning\n",
    "fine_tuned_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary to ensure everything looks good\n",
    "fine_tuned_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "462/462 [==============================] - 57s 116ms/step - loss: 3.5275 - accuracy: 0.0562 - val_loss: 3.5458 - val_accuracy: 0.0325\n",
      "Epoch 2/50\n",
      "462/462 [==============================] - 52s 114ms/step - loss: 3.2033 - accuracy: 0.1431 - val_loss: 4.0324 - val_accuracy: 0.0339\n",
      "Epoch 3/50\n",
      "462/462 [==============================] - 54s 116ms/step - loss: 2.4408 - accuracy: 0.3093 - val_loss: 5.1673 - val_accuracy: 0.0322\n",
      "Epoch 4/50\n",
      "462/462 [==============================] - 55s 120ms/step - loss: 1.8784 - accuracy: 0.4307 - val_loss: 6.4445 - val_accuracy: 0.0320\n",
      "Epoch 5/50\n",
      "462/462 [==============================] - 54s 116ms/step - loss: 1.5894 - accuracy: 0.4989 - val_loss: 6.0282 - val_accuracy: 0.0420\n",
      "Epoch 6/50\n",
      "462/462 [==============================] - 52s 113ms/step - loss: 1.4194 - accuracy: 0.5418 - val_loss: 5.4469 - val_accuracy: 0.0818\n",
      "Epoch 7/50\n",
      "462/462 [==============================] - 58s 126ms/step - loss: 1.3147 - accuracy: 0.5744 - val_loss: 5.6420 - val_accuracy: 0.0902\n",
      "Epoch 8/50\n",
      "462/462 [==============================] - 59s 128ms/step - loss: 1.2242 - accuracy: 0.5982 - val_loss: 5.0157 - val_accuracy: 0.0937\n",
      "Epoch 9/50\n",
      "462/462 [==============================] - 57s 122ms/step - loss: 1.1358 - accuracy: 0.6253 - val_loss: 4.6805 - val_accuracy: 0.1151\n",
      "Epoch 10/50\n",
      "462/462 [==============================] - 60s 131ms/step - loss: 1.0919 - accuracy: 0.6379 - val_loss: 4.6300 - val_accuracy: 0.1111\n",
      "Epoch 11/50\n",
      "462/462 [==============================] - 57s 124ms/step - loss: 1.0348 - accuracy: 0.6573 - val_loss: 3.8389 - val_accuracy: 0.1200\n",
      "Epoch 12/50\n",
      "462/462 [==============================] - 57s 124ms/step - loss: 0.9939 - accuracy: 0.6716 - val_loss: 4.1193 - val_accuracy: 0.1138\n",
      "Epoch 13/50\n",
      "462/462 [==============================] - 56s 121ms/step - loss: 0.9498 - accuracy: 0.6818 - val_loss: 4.0108 - val_accuracy: 0.1102\n",
      "Epoch 14/50\n",
      "462/462 [==============================] - 57s 124ms/step - loss: 0.9193 - accuracy: 0.6934 - val_loss: 3.4075 - val_accuracy: 0.1406\n",
      "Epoch 15/50\n",
      "462/462 [==============================] - 57s 123ms/step - loss: 0.8879 - accuracy: 0.7055 - val_loss: 3.3544 - val_accuracy: 0.1501\n",
      "Epoch 16/50\n",
      "462/462 [==============================] - 57s 122ms/step - loss: 0.8504 - accuracy: 0.7148 - val_loss: 3.3231 - val_accuracy: 0.1441\n",
      "Epoch 17/50\n",
      "462/462 [==============================] - 58s 125ms/step - loss: 0.8195 - accuracy: 0.7212 - val_loss: 2.7696 - val_accuracy: 0.2259\n",
      "Epoch 18/50\n",
      "462/462 [==============================] - 57s 123ms/step - loss: 0.7943 - accuracy: 0.7314 - val_loss: 2.8959 - val_accuracy: 0.1823\n",
      "Epoch 19/50\n",
      "462/462 [==============================] - 55s 118ms/step - loss: 0.7794 - accuracy: 0.7386 - val_loss: 3.4601 - val_accuracy: 0.1343\n",
      "Epoch 20/50\n",
      "462/462 [==============================] - 55s 119ms/step - loss: 0.7549 - accuracy: 0.7470 - val_loss: 2.9608 - val_accuracy: 0.1828\n",
      "Epoch 21/50\n",
      "462/462 [==============================] - 54s 117ms/step - loss: 0.7364 - accuracy: 0.7522 - val_loss: 3.5228 - val_accuracy: 0.1227\n",
      "Epoch 22/50\n",
      "462/462 [==============================] - 52s 113ms/step - loss: 0.7360 - accuracy: 0.7531 - val_loss: 2.7576 - val_accuracy: 0.2002\n",
      "Epoch 23/50\n",
      "462/462 [==============================] - 52s 112ms/step - loss: 0.7128 - accuracy: 0.7570 - val_loss: 2.9323 - val_accuracy: 0.1801\n",
      "Epoch 24/50\n",
      "462/462 [==============================] - 51s 111ms/step - loss: 0.6974 - accuracy: 0.7685 - val_loss: 3.0959 - val_accuracy: 0.1606\n",
      "Epoch 25/50\n",
      "462/462 [==============================] - 52s 112ms/step - loss: 0.6714 - accuracy: 0.7734 - val_loss: 2.8442 - val_accuracy: 0.2124\n",
      "Epoch 26/50\n",
      "462/462 [==============================] - 54s 118ms/step - loss: 0.6685 - accuracy: 0.7724 - val_loss: 2.4017 - val_accuracy: 0.2614\n",
      "Epoch 27/50\n",
      "462/462 [==============================] - 53s 114ms/step - loss: 0.6513 - accuracy: 0.7813 - val_loss: 3.2650 - val_accuracy: 0.1322\n",
      "Epoch 28/50\n",
      "462/462 [==============================] - 53s 116ms/step - loss: 0.6544 - accuracy: 0.7826 - val_loss: 3.1825 - val_accuracy: 0.1528\n",
      "Epoch 29/50\n",
      "462/462 [==============================] - 62s 135ms/step - loss: 0.6366 - accuracy: 0.7883 - val_loss: 2.9307 - val_accuracy: 0.1934\n",
      "Epoch 30/50\n",
      "462/462 [==============================] - 60s 130ms/step - loss: 0.6122 - accuracy: 0.7931 - val_loss: 2.8054 - val_accuracy: 0.1985\n",
      "Epoch 31/50\n",
      "462/462 [==============================] - 63s 137ms/step - loss: 0.6212 - accuracy: 0.7921 - val_loss: 2.7116 - val_accuracy: 0.2153\n",
      "Epoch 32/50\n",
      "462/462 [==============================] - 59s 128ms/step - loss: 0.6012 - accuracy: 0.7980 - val_loss: 2.8996 - val_accuracy: 0.2050\n",
      "Epoch 33/50\n",
      "462/462 [==============================] - 59s 128ms/step - loss: 0.5850 - accuracy: 0.8045 - val_loss: 3.5157 - val_accuracy: 0.1094\n",
      "Epoch 34/50\n",
      "462/462 [==============================] - 56s 122ms/step - loss: 0.5807 - accuracy: 0.8019 - val_loss: 3.2999 - val_accuracy: 0.1647\n",
      "Epoch 35/50\n",
      "462/462 [==============================] - 61s 131ms/step - loss: 0.5878 - accuracy: 0.8004 - val_loss: 3.0526 - val_accuracy: 0.1920\n",
      "Epoch 36/50\n",
      "462/462 [==============================] - 62s 134ms/step - loss: 0.5697 - accuracy: 0.8074 - val_loss: 3.2195 - val_accuracy: 0.1530\n",
      "Epoch 37/50\n",
      "462/462 [==============================] - 55s 119ms/step - loss: 0.5755 - accuracy: 0.8086 - val_loss: 2.8762 - val_accuracy: 0.2126\n",
      "Epoch 38/50\n",
      "462/462 [==============================] - 53s 114ms/step - loss: 0.5542 - accuracy: 0.8108 - val_loss: 2.6840 - val_accuracy: 0.2286\n",
      "Epoch 39/50\n",
      "462/462 [==============================] - 53s 115ms/step - loss: 0.5514 - accuracy: 0.8152 - val_loss: 2.5883 - val_accuracy: 0.2459\n",
      "Epoch 40/50\n",
      "462/462 [==============================] - 50s 109ms/step - loss: 0.5493 - accuracy: 0.8119 - val_loss: 2.4024 - val_accuracy: 0.2684\n",
      "Epoch 41/50\n",
      "462/462 [==============================] - 50s 109ms/step - loss: 0.5338 - accuracy: 0.8185 - val_loss: 2.9327 - val_accuracy: 0.2086\n",
      "Epoch 42/50\n",
      "462/462 [==============================] - 53s 114ms/step - loss: 0.5262 - accuracy: 0.8225 - val_loss: 3.4623 - val_accuracy: 0.1541\n",
      "Epoch 43/50\n",
      "462/462 [==============================] - 56s 122ms/step - loss: 0.5301 - accuracy: 0.8218 - val_loss: 2.8157 - val_accuracy: 0.2313\n",
      "Epoch 44/50\n",
      "462/462 [==============================] - 74s 161ms/step - loss: 0.5158 - accuracy: 0.8255 - val_loss: 2.7977 - val_accuracy: 0.2183\n",
      "Epoch 45/50\n",
      "462/462 [==============================] - 63s 137ms/step - loss: 0.5146 - accuracy: 0.8295 - val_loss: 2.9199 - val_accuracy: 0.2140\n",
      "Epoch 46/50\n",
      "462/462 [==============================] - 63s 136ms/step - loss: 0.5109 - accuracy: 0.8281 - val_loss: 2.7942 - val_accuracy: 0.2224\n",
      "Epoch 47/50\n",
      "462/462 [==============================] - 63s 137ms/step - loss: 0.5075 - accuracy: 0.8267 - val_loss: 2.3844 - val_accuracy: 0.2787\n",
      "Epoch 48/50\n",
      "462/462 [==============================] - 65s 140ms/step - loss: 0.5108 - accuracy: 0.8250 - val_loss: 2.2035 - val_accuracy: 0.2947\n",
      "Epoch 49/50\n",
      "462/462 [==============================] - 66s 143ms/step - loss: 0.5069 - accuracy: 0.8290 - val_loss: 3.0735 - val_accuracy: 0.1793\n",
      "Epoch 50/50\n",
      "462/462 [==============================] - 66s 143ms/step - loss: 0.4947 - accuracy: 0.8320 - val_loss: 3.2661 - val_accuracy: 0.1712\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model on the new dataset\n",
    "history = fine_tuned_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fine_tuned_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fine_tuned_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "fine_tuned_model.save('fine_tuned_model', save_format='tf')  # Saves in the SavedModel format (.pb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
